{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import glob\n",
    "import requests\n",
    "import csv\n",
    "import json\n",
    "import datetime\n",
    "import boto3\n",
    "import shutil\n",
    "import logging\n",
    "from boto3.s3.transfer import S3Transfer\n",
    "import pandas as pd\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_date = datetime.datetime.now().strftime(\"%d%m%Y_%M%S\")\n",
    "\n",
    "logging.basicConfig(filename = log_date + '.txt',\n",
    "                            filemode='a',\n",
    "                            format='%(asctime)s,%(msecs)d %(name)s %(levelname)s %(message)s',\n",
    "                            datefmt='%H:%M:%S',\n",
    "                            level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('config.json') as json_file:\n",
    "    json_txt =json.load(json_file)\n",
    "AWS_ACCESS_KEY = json_txt[\"AWSAccess\"]\n",
    "AWS_SECRET_KEY = json_txt[\"AWSSecret\"]\n",
    "\n",
    "conn = boto3.client('s3', aws_access_key_id=AWS_ACCESS_KEY, aws_secret_access_key=AWS_SECRET_KEY)\n",
    "\n",
    "connResponse = conn.list_buckets()\n",
    "bucket_list = []\n",
    "for bucket in connResponse[\"Buckets\"]:\n",
    "    bucket_list.append(bucket['Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-2cfb8b9dd8a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0minitial_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'*.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mappendedfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minitial_file\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0mdate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mappendedfile\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mmaxdate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'-'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'-'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "if 'Team6ILAssignment01' in bucket_list:\n",
    "    \n",
    "    # If initial large file is present\n",
    "    \n",
    "    with open('config.json') as json_file:    # opening files one by one \n",
    "        json_txt =json.load(json_file)\n",
    "    State=json_txt[\"state\"]\n",
    "    Team =json_txt[\"team\"]\n",
    "    Link =json_txt[\"link\"]\n",
    "    St_Id = json_txt[\"Station_Id\"]\n",
    "    ACCESS_KEY = json_txt[\"AWSAccess\"]\n",
    "    SECRET_KEY = json_txt[\"AWSSecret\"]\n",
    "    \n",
    "    response_new = requests.get(Link)\n",
    "    Data_df_new = pd.read_csv(io.StringIO(response_new.content.decode('utf-8')), low_memory=False)\n",
    "    Data_df_new['Date_Formatted'] = pd.to_datetime(Data_df_new['DATE']).dt.strftime('%d-%m-%Y')\n",
    "    \n",
    "    initial_file = glob.glob('*.csv')\n",
    "    appendedfile = initial_file[0]\n",
    "    date = appendedfile[3:11]\n",
    "    maxdate = date[:4] + '-' + date[4:6] + '-' + date[6:]\n",
    "        \n",
    "    final_df = Data_df_new[(Data_df_new['Date_Formatted'] > maxdate)]\n",
    "    Date_new = max(final_df['DATE'])\n",
    "    date_formatted_new = Date_new[8:10] + Date_new[5:7] + Date_new[:4]\n",
    "    actual_file_new = State + '_' + date_formatted_new + '_' + St_Id + '.csv'\n",
    "    \n",
    "    # Write to .CSV\n",
    "    if not os.path.exists(actual_file_new):\n",
    "        print('Starting new file Download!!!!!', '\\n')\n",
    "        logging.warning(\"Starting new file Download!!!!!\")\n",
    "        with open(appendedfile, \"a\") as f:\n",
    "            f.write(final_df.to_csv(index=False,header=None))\n",
    "        os.rename(appendedfile,actual_file_new)\n",
    "        print('New file Downloaded and appended to Initial File!!!!!', '\\n')\n",
    "        logging.warning(\"New file Downloaded and appended to Initial File!!!!!\")\n",
    "        \n",
    "    else: \n",
    "        logging.warning(\"New file already present!!!!\")\n",
    "        print('New file already present!!!!!','\\n')\n",
    "        \n",
    "else:\n",
    "    \n",
    "    # If initial large file is not present\n",
    "    \n",
    "    with open('Initial_config.json') as json_file:    # opening files one by one \n",
    "        json_txt =json.load(json_file)\n",
    "    State=json_txt[\"state\"]\n",
    "    Team =json_txt[\"team\"]\n",
    "    Link =json_txt[\"link\"]\n",
    "    St_Id = json_txt[\"Station_Id\"]\n",
    "    ACCESS_KEY = json_txt[\"AWSAccess\"]\n",
    "    SECRET_KEY = json_txt[\"AWSSecret\"]\n",
    "    \n",
    "    if not os.path.exists('*.csv'):\n",
    "        print('Starting Initial File Download!!!!!', '\\n')\n",
    "        logging.warning(\"Starting Initial File Download!!!!!\")\n",
    "        response_initial = None\n",
    "        for x in Link:\n",
    "            for z,y in x.items():\n",
    "                response_initial = requests.get(y)\n",
    "                # Write to .CSV\n",
    "                if z == 'link1':\n",
    "                    with open ('Temp.csv', \"a\") as f:\n",
    "                        f.write(response_initial.text)\n",
    "                else:\n",
    "                    with open ('Temp.csv', \"a\") as f:\n",
    "                        for x in response_initial.text.strip().split(\"\\n\")[1:]:\n",
    "                            f.write(x+\"\\n\")\n",
    "                            \n",
    "        Data_df_initial = pd.read_csv(io.StringIO(response_initial.content.decode('utf-8')), low_memory=False)                  \n",
    "        Date_Initial = max(Data_df_initial['DATE'])\n",
    "        date_formatted_initial = Date_Initial[8:10] + Date_Initial[5:7] + Date_Initial[:4]\n",
    "        actual_file_initial = State + '_' + date_formatted_initial + '_' + St_Id + '.csv'\n",
    "        os.rename('Temp.csv', actual_file_initial)\n",
    "                            \n",
    "        print('Ending Initial File Download!!!!!', '\\n')\n",
    "        logging.warning(\"Ending Initial File Download!!!!!\")\n",
    "        \n",
    "    else: \n",
    "        logging.warning(\"Initial File already present!!!!\")\n",
    "        print('Initial File already present!!!!!', '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sync_to_s3(target_dir, bucket_name, AWS_ACCESS_KEY, AWS_SECRET_KEY):\n",
    "    if not os.path.isdir(target_dir):\n",
    "        raise ValueError('target_dir %r not found.' % target_dir)\n",
    "\n",
    "    s3 = boto3.resource('s3', aws_access_key_id=AWS_ACCESS_KEY, aws_secret_access_key=AWS_SECRET_KEY)\n",
    "    conn = boto3.client('s3', aws_access_key_id=AWS_ACCESS_KEY, aws_secret_access_key=AWS_SECRET_KEY)\n",
    "    transfer = S3Transfer(conn)\n",
    "\n",
    "    response = conn.list_buckets()\n",
    "    existent = []\n",
    "    for bucket in response[\"Buckets\"]:\n",
    "        existent.append(bucket['Name'])\n",
    "        \n",
    "    filename = None\n",
    "    file_list = os.listdir(target_dir)\n",
    "    for file in file_list:\n",
    "        if file.endswith('.csv'):\n",
    "            filename = file\n",
    "        \n",
    "    if bucket_name in existent:\n",
    "        filenames = []\n",
    "        for key in conn.list_objects(Bucket=bucket_name)['Contents']:\n",
    "            filenames.append(key['Key']) \n",
    "        \n",
    "        if filename not in filenames:\n",
    "            print('File upload started to s3!!!!!', '\\n')\n",
    "            logging.warning(\"File upload started to s3!!!!!\")\n",
    "            transfer.upload_file(os.path.join(target_dir, filename), bucket_name, filename)\n",
    "            print('File uploaded to s3!!!!!','\\n')\n",
    "            logging.warning(\"File uploaded to s3!!!!!\")\n",
    "            \n",
    "        else:\n",
    "            logging.warning(\"File already exist on s3!!!!\")\n",
    "            print('File already present on s3!!!!!', '\\n')\n",
    "            \n",
    "    else:\n",
    "        conn.create_bucket(Bucket=bucket_name)\n",
    "        print('File upload started to s3!!!!!', '\\n')\n",
    "        logging.warning(\"File upload started to s3!!!!!\")\n",
    "        transfer.upload_file(os.path.join(target_dir, filename), bucket_name, filename)\n",
    "        print('File uploaded to s3!!!!!','\\n')\n",
    "        logging.warning(\"File uploaded to s3!!!!!\")\n",
    "        \n",
    "sync_to_s3('/usr/src/Assignment1', 'Team6ILAssignment01', AWS_ACCESS_KEY, AWS_SECRET_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
